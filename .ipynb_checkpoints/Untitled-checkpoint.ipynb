{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "c69adce3-92d0-438e-8889-f3a495152e5b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Archive:  resume-entities-for-ner.zip\n",
      "  inflating: Entity Recognition in Resumes.json  \n"
     ]
    }
   ],
   "source": [
    "!unzip resume-entities-for-ner.zip"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "f6577afb-b285-4242-9286-bf0bf4c1cb1e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "data = pd.read_csv('UpdatedResumeDataSet.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "d826b696-a4e6-436d-8a90-c44330136ea7",
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Trailing data",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_4264/1065416811.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mdata_json\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread_json\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'Entity Recognition in Resumes.json'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m~/.virtualenvs/cv/lib/python3.8/site-packages/pandas/util/_decorators.py\u001b[0m in \u001b[0;36mwrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    205\u001b[0m                 \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    206\u001b[0m                     \u001b[0mkwargs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mnew_arg_name\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnew_arg_value\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 207\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    208\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    209\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mcast\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mF\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mwrapper\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.virtualenvs/cv/lib/python3.8/site-packages/pandas/util/_decorators.py\u001b[0m in \u001b[0;36mwrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    309\u001b[0m                     \u001b[0mstacklevel\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mstacklevel\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    310\u001b[0m                 )\n\u001b[0;32m--> 311\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    312\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    313\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mwrapper\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.virtualenvs/cv/lib/python3.8/site-packages/pandas/io/json/_json.py\u001b[0m in \u001b[0;36mread_json\u001b[0;34m(path_or_buf, orient, typ, dtype, convert_axes, convert_dates, keep_default_dates, numpy, precise_float, date_unit, encoding, encoding_errors, lines, chunksize, compression, nrows, storage_options)\u001b[0m\n\u001b[1;32m    612\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    613\u001b[0m     \u001b[0;32mwith\u001b[0m \u001b[0mjson_reader\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 614\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mjson_reader\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    615\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    616\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.virtualenvs/cv/lib/python3.8/site-packages/pandas/io/json/_json.py\u001b[0m in \u001b[0;36mread\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    746\u001b[0m                 \u001b[0mobj\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_get_object_parser\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_combine_lines\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata_lines\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    747\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 748\u001b[0;31m             \u001b[0mobj\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_get_object_parser\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    749\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mclose\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    750\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mobj\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.virtualenvs/cv/lib/python3.8/site-packages/pandas/io/json/_json.py\u001b[0m in \u001b[0;36m_get_object_parser\u001b[0;34m(self, json)\u001b[0m\n\u001b[1;32m    768\u001b[0m         \u001b[0mobj\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    769\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mtyp\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m\"frame\"\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 770\u001b[0;31m             \u001b[0mobj\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mFrameParser\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mjson\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mparse\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    771\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    772\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mtyp\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m\"series\"\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mobj\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.virtualenvs/cv/lib/python3.8/site-packages/pandas/io/json/_json.py\u001b[0m in \u001b[0;36mparse\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    883\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    884\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 885\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_parse_no_numpy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    886\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    887\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mobj\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.virtualenvs/cv/lib/python3.8/site-packages/pandas/io/json/_json.py\u001b[0m in \u001b[0;36m_parse_no_numpy\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1138\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0morient\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m\"columns\"\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1139\u001b[0m             self.obj = DataFrame(\n\u001b[0;32m-> 1140\u001b[0;31m                 \u001b[0mloads\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mjson\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mprecise_float\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprecise_float\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1141\u001b[0m             )\n\u001b[1;32m   1142\u001b[0m         \u001b[0;32melif\u001b[0m \u001b[0morient\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m\"split\"\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: Trailing data"
     ]
    }
   ],
   "source": [
    "data_json = pd.read_json('Entity Recognition in Resumes.json')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "0246f0da-b950-4465-9191-8a8e813842f8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"Skills â\\x80¢ R â\\x80¢ Python â\\x80¢ SAP HANA â\\x80¢ Tableau â\\x80¢ SAP HANA SQL â\\x80¢ SAP HANA PAL â\\x80¢ MS SQL â\\x80¢ SAP Lumira â\\x80¢ C# â\\x80¢ Linear Programming â\\x80¢ Data Modelling â\\x80¢ Advance Analytics â\\x80¢ SCM Analytics â\\x80¢ Retail Analytics â\\x80¢Social Media Analytics â\\x80¢ NLP Education Details \\r\\nJanuary 2017 to January 2018 PGDM Business Analytics  Great Lakes Institute of Management & Illinois Institute of Technology\\r\\nJanuary 2013 Bachelor of Engineering Electronics and Communication Bengaluru, Karnataka New Horizon College of Engineering, Bangalore Visvesvaraya Technological University\\r\\nData Science Consultant \\r\\n\\r\\nConsultant - Deloitte USI\\r\\nSkill Details \\r\\nLINEAR PROGRAMMING- Exprience - 6 months\\r\\nRETAIL- Exprience - 6 months\\r\\nRETAIL MARKETING- Exprience - 6 months\\r\\nSCM- Exprience - 6 months\\r\\nSQL- Exprience - Less than 1 year months\\r\\nDeep Learning- Exprience - Less than 1 year months\\r\\nMachine learning- Exprience - Less than 1 year months\\r\\nPython- Exprience - Less than 1 year months\\r\\nR- Exprience - Less than 1 year monthsCompany Details \\r\\ncompany - Deloitte USI\\r\\ndescription - The project involved analysing historic deals and coming with insights to optimize future deals.\\r\\nRole: Was given raw data, carried out end to end analysis and presented insights to client.\\r\\nKey Responsibilities:\\r\\nâ\\x80¢ Extract data from client systems across geographies.\\r\\nâ\\x80¢ Understand and build reports in tableau. Infer meaningful insights to optimize prices and find out process blockades.\\r\\nTechnical Environment: R, Tableau.\\r\\n\\r\\nIndustry: Cross Industry\\r\\nService Area: Cross Industry - Products\\r\\nProject Name: Handwriting recognition\\r\\nConsultant: 3 months.\\r\\nThe project involved taking handwritten images and converting them to digital text images by object detection and sentence creation.\\r\\nRole: I was developing sentence correction functionality.\\r\\nKey Responsibilities:\\r\\nâ\\x80¢ Gather data large enough to capture all English words\\r\\nâ\\x80¢ Train LSTM models on words.\\r\\nTechnical Environment: Python.\\r\\n\\r\\nIndustry: Finance\\r\\nService Area: Financial Services - BI development Project Name: SWIFT\\r\\nConsultant: 8 months.\\r\\nThe project was to develop an analytics infrastructure on top of SAP S/4, it would user to view\\r\\nfinancial reports to respective departments. Reporting also included forecasting expenses.\\r\\nRole: I was leading the offshore team.\\r\\nKey Responsibilities:\\r\\nâ\\x80¢ Design & Develop data models for reporting.\\r\\nâ\\x80¢ Develop ETL for data flow\\r\\nâ\\x80¢ Validate various reports.\\r\\nTechnical Environment: SAP HANA, Tableau, SAP AO.\\r\\n\\r\\nIndustry: Healthcare Analytics\\r\\nService Area: Life Sciences - Product development Project Name: Clinical Healthcare System\\r\\nConsultant: 2 months.\\r\\nThe project was to develop an analytics infrastructure on top of Argus, it would allow users to query faster and provide advance analytics capabilities.\\r\\nRole: I was involved from design to deploy phase, performed a lot of data restructuring and built\\r\\nmodels for insights.\\r\\nKey Responsibilities:\\r\\nâ\\x80¢ Design & Develop data models for reporting.\\r\\nâ\\x80¢ Develop and deploy analytical models.\\r\\nâ\\x80¢ Validate various reports.\\r\\nTechnical Environment: Data Modelling, SAP HANA, Tableau, NLP.\\r\\n\\r\\nIndustry: FMCG\\r\\nService Area: Trade & Promotion\\r\\nProject Name: Consumption Based Planning for Flowers Foods Consultant; 8 months.\\r\\nThe project involved setting up of CRM and CBP modules.\\r\\nRole: I was involved in key data decomposition activities and setting up the base for future year\\r\\nforecast. Over the course of the project I developed various models and carried out key\\r\\nperformance improvements.\\r\\nKey Responsibilities:\\r\\nâ\\x80¢ Design & Develop HANA models for decomposition.\\r\\nâ\\x80¢ Develop data flow for forecast.\\r\\nâ\\x80¢ Developed various views for reporting of Customer/Sales/Funds.\\r\\nâ\\x80¢ Validate various reports in BOBJ.\\r\\nTechnical Environment: Data Modelling, SAP HANA, BOBJ, Time Series Forecasting.\\r\\n\\r\\nInternal Initiative Industry: FMCG\\r\\nCustomer Segmentation and RFM analysis Consultant; 3 months.\\r\\nThe initiative involved setting up of HANA-Python interface and advance analytics on Python. Over the course I had successfully segmented data into five core segments using K-means and carried out RFM analysis in Python. Also developed algorithm to categorize any new customer under the defined buckets.\\r\\nTechnical Environment: Anaconda3, Python3.6, HANA SPS12\\r\\n\\r\\nIndustry: Telecom Invoice state detection Consultant; 1 months.\\r\\nThe initiative was to reduce the manual effort in verifying closed and open invoices manually, it\\r\\ninvolved development to a decision tree to classify open/closed invoices. This enabled effort\\r\\nreduction by 60%.\\r\\nTechnical Environment: R, SAP PAL, SAP HANA SPS12\\r\\n\\r\\nAccenture Experience\\r\\nIndustry: Analytics - Cross Industry\\r\\nIn Process Analytics for SAP Senior Developer; 19 months.\\r\\nAccenture Solutions Pvt. Ltd., India\\r\\nThe project involved development of SAP analytics tool - In Process Analytics (IPA) . My role was to develop database objects and data models to provide operational insights to clients.\\r\\nRole: I have developed various Finance related KPIs and spearheaded various deployments.\\r\\nIntroduced SAP Predictive analytics to reduce development time and reuse functionalities for KPIs and prepared production planning reports.\\r\\nKey Responsibilities:\\r\\nâ\\x80¢ Involved in information gather phase.\\r\\nâ\\x80¢ Designed and implemented SAP HANA data modelling using Attribute View, Analytic View, and\\r\\nCalculation View.\\r\\nâ\\x80¢ Developed various KPI's individually using complex SQL scripts in Calculation views.\\r\\nâ\\x80¢ Created procedures in HANA Database.\\r\\nâ\\x80¢ Took ownership and developed Dashboard functionality.\\r\\nâ\\x80¢ Involved in building data processing algorithms to be executed in R server for cluster analysis.\\r\\nTechnical Environment: R, SAP HANA, T-SQL.\\r\\nIndustry: Cross Industry\\r\\nAccenture Testing Accelerator for SAP Database Developer; 21 months.\\r\\nAccenture Solutions Pvt. Ltd., India\\r\\nRole: I have taken care of all development activities for the ATAS tool and have also completed\\r\\nvarious deployments of the product.\\r\\nApart from these activities I was also actively involved in maintenance of the database servers\\r\\n(Production & Quality)\\r\\nKey Responsibilities:\\r\\nâ\\x80¢ Analyzing business requirements, understanding the scope, getting requirements clarified\\r\\ninteracting with business and further transform all requirements to generate attribute\\r\\nmapping documents and reviewing mapping specification documentation\\r\\nâ\\x80¢ Create / Update database objects like tables, views, stored procedures, function, and packages\\r\\nâ\\x80¢ Monitored SQL Server Error Logs and Application Logs through SQL Server Agent\\r\\nâ\\x80¢ Prepared Data Flow Diagrams, Entity Relationship Diagrams using UML\\r\\nâ\\x80¢ Responsible for Designing, developing and Normalization of database tables\\r\\nâ\\x80¢ Experience in performance tuning using SQL profiler.\\r\\nâ\\x80¢ Involved in QA, UAT, knowledge transfer and support activities\\r\\nTechnical Environment: SQL Server 2008/2014, Visual Studio 2010, Windows Server, Performance\\r\\nMonitor, SQL Server Profiler, C#, PL-SQL, T-SQL.\""
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.iloc[3].Resume"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b7f4ec98-eb68-4f46-bffc-e028d579a614",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "cv",
   "language": "python",
   "name": "cv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
